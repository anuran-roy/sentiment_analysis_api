>>>  ï»  ~  python            
Python 3.10.6 (main, Nov  2 2022, 18:53:38) [GCC 11.3.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
>>> from sklearn.ensemble import RandomForestClassifier as rfc
>>> import nltk
>>> import rich
>>> rich.print(dir(nltk))
[
    'ARLSTem',
    'ARLSTem2',
    'AbstractLazySequence',
    'AffixTagger',
    'AlignedSent',
    'Alignment',
    'AnnotationTask',
    'ApplicationExpression',
    'Assignment',
    'BigramAssocMeasures',
    'BigramCollocationFinder',
    'BigramTagger',
    'BinaryMaxentFeatureEncoding',
    'BlanklineTokenizer',
    'BllipParser',
    'BottomUpChartParser',
    'BottomUpLeftCornerChartParser',
    'BottomUpProbabilisticChartParser',
    'Boxer',
    'BrillTagger',
    'BrillTaggerTrainer',
    'CFG',
    'CRFTagger',
    'CfgReadingCommand',
    'ChartParser',
    'ChunkParserI',
    'ChunkScore',
    'Cistem',
    'ClassifierBasedPOSTagger',
    'ClassifierBasedTagger',
    'ClassifierI',
    'ConcordanceIndex',
    'ConditionalExponentialClassifier',
    'ConditionalFreqDist',
    'ConditionalProbDist',
    'ConditionalProbDistI',
    'ConfusionMatrix',
    'ContextIndex',
    'ContextTagger',
    'ContingencyMeasures',
    'CoreNLPDependencyParser',
    'CoreNLPParser',
    'Counter',
    'CrossValidationProbDist',
    'DRS',
    'DecisionTreeClassifier',
    'DefaultTagger',
    'DependencyEvaluator',
    'DependencyGrammar',
    'DependencyGraph',
    'DependencyProduction',
    'DictionaryConditionalProbDist',
    'DictionaryProbDist',
    'DiscourseTester',
    'DrtExpression',
    'DrtGlueReadingCommand',
    'ELEProbDist',
    'EarleyChartParser',
    'Expression',
    'FStructure',
    'FeatDict',
    'FeatList',
    'FeatStruct',
    'FeatStructReader',
    'Feature',
    'FeatureBottomUpChartParser',
    'FeatureBottomUpLeftCornerChartParser',
    'FeatureChartParser',
    'FeatureEarleyChartParser',
    'FeatureIncrementalBottomUpChartParser',
    'FeatureIncrementalBottomUpLeftCornerChartParser',
    'FeatureIncrementalChartParser',
    'FeatureIncrementalTopDownChartParser',
    'FeatureTopDownChartParser',
    'FreqDist',
    'HTTPPasswordMgrWithDefaultRealm',
    'HeldoutProbDist',
    'HiddenMarkovModelTagger',
    'HiddenMarkovModelTrainer',
    'HunposTagger',
    'IBMModel',
    'IBMModel1',
    'IBMModel2',
    'IBMModel3',
    'IBMModel4',
    'IBMModel5',
    'ISRIStemmer',
    'ImmutableMultiParentedTree',
    'ImmutableParentedTree',
    'ImmutableProbabilisticMixIn',
    'ImmutableProbabilisticTree',
    'ImmutableTree',
    'IncrementalBottomUpChartParser',
    'IncrementalBottomUpLeftCornerChartParser',
    'IncrementalChartParser',
    'IncrementalLeftCornerChartParser',
    'IncrementalTopDownChartParser',
    'Index',
    'InsideChartParser',
    'JSONTaggedDecoder',
    'JSONTaggedEncoder',
    'KneserNeyProbDist',
    'LancasterStemmer',
    'LaplaceProbDist',
    'LazyConcatenation',
    'LazyEnumerate',
    'LazyIteratorList',
    'LazyMap',
    'LazySubsequence',
    'LazyZip',
    'LeftCornerChartParser',
    'LegalitySyllableTokenizer',
    'LidstoneProbDist',
    'LineTokenizer',
    'LogicalExpressionException',
    'LongestChartParser',
    'MLEProbDist',
    'MWETokenizer',
    'Mace',
    'MaceCommand',
    'MaltParser',
    'MaxentClassifier',
    'Model',
    'MultiClassifierI',
    'MultiParentedTree',
    'MutableProbDist',
    'NLTKWordTokenizer',
    'NaiveBayesClassifier',
    'NaiveBayesDependencyScorer',
    'NgramAssocMeasures',
    'NgramTagger',
    'NonprojectiveDependencyParser',
    'Nonterminal',
    'OrderedDict',
    'PCFG',
    'Paice',
    'ParallelProverBuilder',
    'ParallelProverBuilderCommand',
    'ParentedTree',
    'ParserI',
    'PerceptronTagger',
    'PhraseTable',
    'PorterStemmer',
    'PositiveNaiveBayesClassifier',
    'ProbDistI',
    'ProbabilisticDependencyGrammar',
    'ProbabilisticMixIn',
    'ProbabilisticNonprojectiveParser',
    'ProbabilisticProduction',
    'ProbabilisticProjectiveDependencyParser',
    'ProbabilisticTree',
    'Production',
    'ProjectiveDependencyParser',
    'Prover9',
    'Prover9Command',
    'ProxyBasicAuthHandler',
    'ProxyDigestAuthHandler',
    'ProxyHandler',
    'PunktSentenceTokenizer',
    'QuadgramAssocMeasures',
    'QuadgramCollocationFinder',
    'RSLPStemmer',
    'RTEFeatureExtractor',
    'RUS_PICKLE',
    'RandomChartParser',
    'RangeFeature',
    'ReadingCommand',
    'RecursiveDescentParser',
    'RegexpChunkParser',
    'RegexpParser',
    'RegexpStemmer',
    'RegexpTagger',
    'RegexpTokenizer',
    'ReppTokenizer',
    'ResolutionProver',
    'ResolutionProverCommand',
    'SExprTokenizer',
    'SLASH',
    'Senna',
    'SennaChunkTagger',
    'SennaNERTagger',
    'SennaTagger',
    'SequentialBackoffTagger',
    'ShiftReduceParser',
    'SimpleGoodTuringProbDist',
    'SklearnClassifier',
    'SlashFeature',
    'SnowballStemmer',
    'SpaceTokenizer',
    'StackDecoder',
    'StanfordNERTagger',
    'StanfordPOSTagger',
    'StanfordSegmenter',
    'StanfordTagger',
    'StemmerI',
    'SteppingChartParser',
    'SteppingRecursiveDescentParser',
    'SteppingShiftReduceParser',
    'SyllableTokenizer',
    'TYPE',
    'TabTokenizer',
    'TableauProver',
    'TableauProverCommand',
    'TaggerI',
    'TestGrammar',
    'Text',
    'TextCat',
    'TextCollection',
    'TextTilingTokenizer',
    'TnT',
    'TokenSearcher',
    'ToktokTokenizer',
    'TopDownChartParser',
    'TransitionParser',
    'Tree',
    'TreePrettyPrinter',
    'TreebankWordDetokenizer',
    'TreebankWordTokenizer',
    'Trie',
    'TrigramAssocMeasures',
    'TrigramCollocationFinder',
    'TrigramTagger',
    'TweetTokenizer',
    'TypedMaxentFeatureEncoding',
    'Undefined',
    'UniformProbDist',
    'UnigramTagger',
    'UnsortedChartParser',
    'Valuation',
    'Variable',
    'ViterbiParser',
    'WekaClassifier',
    'WhitespaceTokenizer',
    'WittenBellProbDist',
    'WordNetLemmatizer',
    'WordPunctTokenizer',
    '__author__',
    '__author_email__',
    '__builtins__',
    '__cached__',
    '__classifiers__',
    '__copyright__',
    '__doc__',
    '__file__',
    '__keywords__',
    '__license__',
    '__loader__',
    '__longdescr__',
    '__maintainer__',
    '__maintainer_email__',
    '__name__',
    '__package__',
    '__path__',
    '__spec__',
    '__url__',
    '__version__',
    'accuracy',
    'acyclic_branches_depth_first',
    'acyclic_breadth_first',
    'acyclic_depth_first',
    'acyclic_dic2tree',
    'add_logs',
    'agreement',
    'align',
    'alignment_error_rate',
    'aline',
    'api',
    'app',
    'apply_features',
    'approxrand',
    'arity',
    'arlstem',
    'arlstem2',
    'association',
    'bigrams',
    'binary_distance',
    'binary_search_file',
    'binding_ops',
    'bisect',
    'blankline_tokenize',
    'bleu',
    'bleu_score',
    'bllip',
    'boolean_ops',
    'boxer',
    'bracket_parse',
    'breadth_first',
    'brill',
    'brill_trainer',
    'build_opener',
    'call_megam',
    'casual',
    'casual_tokenize',
    'ccg',
    'chain',
    'chart',
    'chat',
    'chomsky_normal_form',
    'choose',
    'chrf',
    'chrf_score',
    'chunk',
    'cistem',
    'classify',
    'clause',
    'clean_html',
    'clean_url',
    'cluster',
    'collapse_unary',
    'collections',
    'collocations',
    'combinations',
    'compat',
    'config_java',
    'config_megam',
    'config_weka',
    'conflicts',
    'confusionmatrix',
    'conllstr2tree',
    'conlltags2tree',
    'corenlp',
    'corpus',
    'crf',
    'custom_distance',
    'data',
    'decisiontree',
    'decorator',
    'decorators',
    'defaultdict',
    'demo',
    'dependencygraph',
    'deprecated',
    'deque',
    'destructive',
    'discourse',
    'distance',
    'download',
    'download_gui',
    'download_shell',
    'downloader',
    'draw',
    'drt',
    'earleychart',
    'edge_closure',
    'edges2dot',
    'edit_distance',
    'edit_distance_align',
    'elementtree_indent',
    'entropy',
    'equality_preds',
    'evaluate',
    'evaluate_sents',
    'everygrams',
    'extract',
    'extract_rels',
    'extract_test_sentences',
    'f_measure',
    'featstruct',
    'featurechart',
    'filestring',
    'find',
    'flatten',
    'fractional_presence',
    'gale_church',
    'gdfa',
    'getproxies',
    'ghd',
    'gleu',
    'gleu_score',
    'glue',
    'grammar',
    'grow_diag_final_and',
    'guess_encoding',
    'help',
    'hmm',
    'hunpos',
    'ibm1',
    'ibm2',
    'ibm3',
    'ibm4',
    'ibm5',
    'ibm_model',
    'ieerstr2tree',
    'immutable',
    'in_idle',
    'induce_pcfg',
    'inference',
    'infile',
    'inspect',
    'install_opener',
    'internals',
    'interpret_sents',
    'interval_distance',
    'invert_dict',
    'invert_graph',
    'is_rel',
    'islice',
    'isri',
    'jaccard_distance',
    'json_tags',
    'jsontags',
    'lancaster',
    'lazyimport',
    'legality_principle',
    'lfg',
    'line_tokenize',
    'linearlogic',
    'lm',
    'load',
    'load_parser',
    'locale',
    'log_likelihood',
    'logic',
    'mace',
    'malt',
    'map_tag',
    'mapping',
    'masi_distance',
    'maxent',
    'megam',
    'memoize',
    'meteor',
    'meteor_score',
    'metrics',
    'misc',
    'mwe',
    'naivebayes',
    'ne_chunk',
    'ne_chunk_sents',
    'ngrams',
    'nist',
    'nist_score',
    'nonprojectivedependencyparser',
    'nonterminals',
    'numpy',
    'os',
    'pad_sequence',
    'paice',
    'pairwise',
    'parallelize_preprocess',
    'parented',
    'parse',
    'parse_sents',
    'parsing',
    'pchart',
    'perceptron',
    'phrase_based',
    'pk',
    'porter',
    'pos_tag',
    'pos_tag_sents',
    'positivenaivebayes',
    'pprint',
    'pr',
    'precision',
    'presence',
    'prettyprinter',
    'print_string',
    'probabilistic',
    'probability',
    'projectivedependencyparser',
    'prover9',
    'punkt',
    'pydoc',
    'raise_unorderable_types',
    'ranks_from_scores',
    'ranks_from_sequence',
    're',
    're_show',
    'read_grammar',
    'read_logic',
    'read_valuation',
    'recall',
    'recursivedescent',
    'regexp',
    'regexp_span_tokenize',
    'regexp_tokenize',
    'register_tag',
    'relextract',
    'repp',
    'resolution',
    'ribes',
    'ribes_score',
    'root_semrep',
    'rslp',
    'rte_classifier',
    'rte_classify',
    'rte_features',
    'rtuple',
    'scikitlearn',
    'scores',
    'segmentation',
    'sem',
    'senna',
    'sent_tokenize',
    'sequential',
    'set2rel',
    'set_proxy',
    'sexpr',
    'sexpr_tokenize',
    'shiftreduce',
    'simple',
    'sinica_parse',
    'skipgrams',
    'skolemize',
    'slice_bounds',
    'snowball',
    'sonority_sequencing',
    'spearman',
    'spearman_correlation',
    'stack_decoder',
    'stanford',
    'stanford_segmenter',
    'stem',
    'str2tuple',
    'string_span_tokenize',
    'subprocess',
    'subsumes',
    'sum_logs',
    'sys',
    'tableau',
    'tadm',
    'tag',
    'tagset_mapping',
    'tagstr2tree',
    'tbl',
    'tee',
    'text',
    'textcat',
    'texttiling',
    'textwrap',
    'tkinter',
    'tnt',
    'tokenize',
    'tokenwrap',
    'toktok',
    'toolbox',
    'total_ordering',
    'trace',
    'transforms',
    'transitionparser',
    'transitive_closure',
    'translate',
    'tree',
    'tree2conllstr',
    'tree2conlltags',
    'treebank',
    'trigrams',
    'tuple2str',
    'types',
    'un_chomsky_normal_form',
    'unify',
    'unique_list',
    'untag',
    'unweighted_minimum_spanning_dict',
    'unweighted_minimum_spanning_digraph',
    'unweighted_minimum_spanning_tree',
    'usage',
    'util',
    'version_file',
    'viterbi',
    'warnings',
    'weka',
    'windowdiff',
    'word_tokenize',
    'wordnet',
    'wordpunct_tokenize',
    'wsd'
]
>>> sentence = "plus you've added commercials to the experience... tacky."
>>> wordnet = WordNetLemmatizer()
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'WordNetLemmatizer' is not defined
>>> wordnet = nltk.WordNetLemmatizer()
>>> dir(wordnet)
['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'lemmatize']
>>> wordnet.lemmatize(sentence)
"plus you've added commercials to the experience... tacky."
>>> wordnet.lemmatize(sentence.split())
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/home/anuran/.local/lib/python3.10/site-packages/nltk/stem/wordnet.py", line 45, in lemmatize
    lemmas = wn._morphy(word, pos)
  File "/home/anuran/.local/lib/python3.10/site-packages/nltk/corpus/reader/wordnet.py", line 2032, in _morphy
    if form in exceptions:
TypeError: unhashable type: 'list'
>>> words = sentence.split()
>>> lemmatized_words = [wordnet.lemmatize(x) for x in words]
>>> lemmatized_words
['plus', "you've", 'added', 'commercial', 'to', 'the', 'experience...', 'tacky.']
>>> sentence_tokenizer = nltk.PunktSentenceTokenizer()
>>> dir(sentence_tokenizer)
['PUNCTUATION', '_Token', '__abstractmethods__', '__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '__weakref__', '_abc_impl', '_annotate_first_pass', '_annotate_second_pass', '_annotate_tokens', '_build_sentence_list', '_first_pass_annotation', '_lang_vars', '_match_potential_end_contexts', '_ortho_heuristic', '_params', '_realign_boundaries', '_second_pass_annotation', '_slices_from_text', '_tokenize_words', 'debug_decisions', 'dump', 'sentences_from_text', 'sentences_from_text_legacy', 'sentences_from_tokens', 'span_tokenize', 'span_tokenize_sents', 'text_contains_sentbreak', 'tokenize', 'tokenize_sents', 'train']
>>> sentence_tokenizer.span_tokenize(sentence)
<generator object PunktSentenceTokenizer.span_tokenize at 0x7eff41bbc970>
>>> list(sentence_tokenizer.span_tokenize(sentence))
[(0, 57)]
>>> list(sentence_tokenizer._tokenize_words(sentence))
[PunktToken('plus', linestart=True), PunktToken('you', ), PunktToken("'ve", ), PunktToken('added', ), PunktToken('commercials', ), PunktToken('to', ), PunktToken('the', ), PunktToken('experience', ), PunktToken('...', ), PunktToken('tacky.', )]
>>> list(sentence_tokenizer.tokenize_sents(sentence))
[['p'], ['l'], ['u'], ['s'], [], ['y'], ['o'], ['u'], ["'"], ['v'], ['e'], [], ['a'], ['d'], ['d'], ['e'], ['d'], [], ['c'], ['o'], ['m'], ['m'], ['e'], ['r'], ['c'], ['i'], ['a'], ['l'], ['s'], [], ['t'], ['o'], [], ['t'], ['h'], ['e'], [], ['e'], ['x'], ['p'], ['e'], ['r'], ['i'], ['e'], ['n'], ['c'], ['e'], ['.'], ['.'], ['.'], [], ['t'], ['a'], ['c'], ['k'], ['y'], ['.']]
>>> list(sentence_tokenizer.tokenize(sentence))
["plus you've added commercials to the experience... tacky."]
>>> list(sentence_tokenizer._tokenize_words(sentence))
[PunktToken('plus', linestart=True), PunktToken('you', ), PunktToken("'ve", ), PunktToken('added', ), PunktToken('commercials', ), PunktToken('to', ), PunktToken('the', ), PunktToken('experience', ), PunktToken('...', ), PunktToken('tacky.', )]
>>> tok = list(sentence_tokenizer._tokenize_words(sentence))
>>> dir(tok[0])
['_RE_ALPHA', '_RE_ELLIPSIS', '_RE_INITIAL', '_RE_NUMERIC', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__slots__', '__str__', '__subclasshook__', '_get_type', '_properties', 'abbr', 'ellipsis', 'first_case', 'first_lower', 'first_upper', 'is_alpha', 'is_ellipsis', 'is_initial', 'is_non_punct', 'is_number', 'linestart', 'parastart', 'period_final', 'sentbreak', 'tok', 'type', 'type_no_period', 'type_no_sentperiod']
>>> tok[0].tok
'plus'
>>> [i.tok for i in tok]
['plus', 'you', "'ve", 'added', 'commercials', 'to', 'the', 'experience', '...', 'tacky.']
>>> [wordnet.lemmatize(i.tok) for i in tok]
['plus', 'you', "'ve", 'added', 'commercial', 'to', 'the', 'experience', '...', 'tacky.']
>>> [wordnet.lemmatize(i.tok) for i in tok if i.is_non_punct]
['plus', 'you', "'ve", 'added', 'commercial', 'to', 'the', 'experience', 'tacky.']
>>>dir
<built-in function dir>
>>> dir(nltk.text)
['BigramAssocMeasures', 'BigramCollocationFinder', 'CFD', 'ConcordanceIndex', 'ConcordanceLine', 'ContextIndex', 'Counter', 'FreqDist', 'LazyConcatenation', 'MLE', 'Text', 'TextCollection', 'TokenSearcher', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'defaultdict', 'demo', 'f_measure', 'log', 'namedtuple', 'padded_everygram_pipeline', 're', 'reduce', 'sent_tokenize', 'sys', 'tokenwrap']
>>> dir(nltk.text.sent_tokenize(sentence))
['__add__', '__class__', '__class_getitem__', '__contains__', '__delattr__', '__delitem__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__imul__', '__init__', '__init_subclass__', '__iter__', '__le__', '__len__', '__lt__', '__mul__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rmul__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'append', 'clear', 'copy', 'count', 'extend', 'index', 'insert', 'pop', 'remove', 'reverse', 'sort']
>>> nltk.text.sent_tokenize(sentence)
["plus you've added commercials to the experience... tacky."]
>>> 
