{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torchtext in /home/anuran/.local/lib/python3.10/site-packages (0.14.1)\n",
      "Requirement already satisfied: torch==1.13.1 in /home/anuran/.local/lib/python3.10/site-packages (from torchtext) (1.13.1)\n",
      "Requirement already satisfied: requests in /home/anuran/.local/lib/python3.10/site-packages (from torchtext) (2.28.0)\n",
      "Requirement already satisfied: numpy in /home/anuran/.local/lib/python3.10/site-packages (from torchtext) (1.22.4)\n",
      "Requirement already satisfied: tqdm in /home/anuran/.local/lib/python3.10/site-packages (from torchtext) (4.64.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/anuran/.local/lib/python3.10/site-packages (from torch==1.13.1->torchtext) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/anuran/.local/lib/python3.10/site-packages (from torch==1.13.1->torchtext) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/anuran/.local/lib/python3.10/site-packages (from torch==1.13.1->torchtext) (11.7.99)\n",
      "Requirement already satisfied: typing-extensions in /home/anuran/.local/lib/python3.10/site-packages (from torch==1.13.1->torchtext) (4.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/anuran/.local/lib/python3.10/site-packages (from torch==1.13.1->torchtext) (11.7.99)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchtext) (0.37.1)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from nvidia-cublas-cu11==11.10.3.66->torch==1.13.1->torchtext) (59.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->torchtext) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anuran/.local/lib/python3.10/site-packages (from requests->torchtext) (2022.6.15)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/anuran/.local/lib/python3.10/site-packages (from requests->torchtext) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/anuran/.local/lib/python3.10/site-packages (from requests->torchtext) (1.26.9)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/anuran/.local/lib/python3.10/site-packages (2.7.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (2.28.0)\n",
      "Requirement already satisfied: pyarrow>=6.0.0 in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (10.0.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (1.22.4)\n",
      "Requirement already satisfied: multiprocess in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: aiohttp in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.11.1 in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (2022.11.0)\n",
      "Requirement already satisfied: pandas in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (1.4.2)\n",
      "Requirement already satisfied: responses<0.19 in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (0.18.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (0.11.1)\n",
      "Requirement already satisfied: xxhash in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (3.1.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: dill<0.3.7 in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /home/anuran/.local/lib/python3.10/site-packages (from datasets) (4.64.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/anuran/.local/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /home/anuran/.local/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/anuran/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/anuran/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /home/anuran/.local/lib/python3.10/site-packages (from aiohttp->datasets) (2.0.12)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/anuran/.local/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp->datasets) (21.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/anuran/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.2.0)\n",
      "Requirement already satisfied: filelock in /home/anuran/.local/lib/python3.10/site-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/anuran/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.19.0->datasets) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/anuran/.local/lib/python3.10/site-packages (from requests>=2.19.0->datasets) (1.26.9)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /home/anuran/.local/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas->datasets) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torchtext\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly as pt\n",
    "import plotly.express as px\n",
    "import nltk\n",
    "import torchtext\n",
    "import datasets \n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "sys.path.append(Path(os.path.abspath(\".\")).parent.parent.absolute())\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/airline_sentiment_analysis.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0 airline_sentiment  \\\n",
       "0           1          positive   \n",
       "1           3          negative   \n",
       "2           4          negative   \n",
       "3           5          negative   \n",
       "4           6          positive   \n",
       "\n",
       "                                                text  \n",
       "0  @VirginAmerica plus you've added commercials t...  \n",
       "1  @VirginAmerica it's really aggressive to blast...  \n",
       "2  @VirginAmerica and it's a really big bad thing...  \n",
       "3  @VirginAmerica seriously would pay $30 a fligh...  \n",
       "4  @VirginAmerica yes, nearly every time I fly VX...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(labels=[\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0          positive  @VirginAmerica plus you've added commercials t...\n",
       "1          negative  @VirginAmerica it's really aggressive to blast...\n",
       "2          negative  @VirginAmerica and it's a really big bad thing...\n",
       "3          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "4          positive  @VirginAmerica yes, nearly every time I fly VX..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trying to understand the nature of the dataset\n",
    "\n",
    "This can mean various points - whether the dataset vocabulary is large, what is the word frequency distribution and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the vocabulary size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.count of       airline_sentiment                                               text\n",
       "0              positive  @VirginAmerica plus you've added commercials t...\n",
       "1              negative  @VirginAmerica it's really aggressive to blast...\n",
       "2              negative  @VirginAmerica and it's a really big bad thing...\n",
       "3              negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "4              positive  @VirginAmerica yes, nearly every time I fly VX...\n",
       "...                 ...                                                ...\n",
       "11536          negative  @AmericanAir my flight was Cancelled Flightled...\n",
       "11537          negative         @AmericanAir right on cue with the delays👌\n",
       "11538          positive  @AmericanAir thank you we got on a different f...\n",
       "11539          negative  @AmericanAir leaving over 20 minutes Late Flig...\n",
       "11540          negative  @AmericanAir you have my money, you change my ...\n",
       "\n",
       "[11541 rows x 2 columns]>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count  # Number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "airline_sentiment    18.524651\n",
       "text                 18.524651\n",
       "dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([len(sentence.split()) for sentence in df[\"text\"]])/df.count()  # Average number of words per sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22602 215542\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "words = [word.lower() for sentence in df[\"text\"] for word in sentence.split(\" \")]\n",
    "unique_words = list(set(words))  # Number of unique words in the dataset\n",
    "print(len(unique_words), len(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, there are 22602 unique words in 23082 sentences, with each sentence containing around 9 words in average. This indicates a huge variety in the vocabulary.\n",
    "Thus we need to concentrate on models that works well with low probabilities and small datasets.\n",
    "\n",
    "Thus, we will use the Skipgram Model to to generate word embeddings for the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_freq_dict = {word: 0 for word in unique_words}\n",
    "word_freq_dict_df = {\"word\": [], \"freq\": []}\n",
    "\n",
    "for word in words:\n",
    "    word_freq_dict[word] += 1\n",
    "\n",
    "for word, freq in word_freq_dict.items():\n",
    "    word_freq_dict_df[\"word\"].append(word)\n",
    "    word_freq_dict_df[\"freq\"].append(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td>1809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>hey...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4439</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>num</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flight🍸#sfo</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22597</th>\n",
       "      <td>captured,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22598</th>\n",
       "      <td>each.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22599</th>\n",
       "      <td>bucks</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22600</th>\n",
       "      <td>495</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22601</th>\n",
       "      <td>md</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22602 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              word  freq\n",
       "0                   1809\n",
       "1           hey...     1\n",
       "2             4439     1\n",
       "3              num     1\n",
       "4      flight🍸#sfo     1\n",
       "...            ...   ...\n",
       "22597    captured,     1\n",
       "22598        each.     1\n",
       "22599        bucks     4\n",
       "22600          495     2\n",
       "22601           md     1\n",
       "\n",
       "[22602 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(word_freq_dict_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['we', \"'re\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"we're\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'ca', \"n't\", '...', 'do', 'this']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "wnl = WordNetLemmatizer()\n",
    "\n",
    "list(map(wnl.lemmatize, word_tokenize(\"I can't... do this\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from typing import List, Dict, Any, Optional, Union, Tuple\n",
    "import torch.functional as F\n",
    "import torch\n",
    "\n",
    "class Word2VecSkipgramModel(nn.Module):\n",
    "    \"\"\"This model learns word embeddings from the Twitter corpus using the Skipgram technique.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocabulary_size: int,\n",
    "        embedding_size: int,\n",
    "        *args: Optional[List[Any]],\n",
    "        **kwargs: Optional[Dict[str, Any]]\n",
    "    ):\n",
    "        super(Word2VecSkipgramModel, self).__init__(*args, **kwargs)\n",
    "        self.vocabulary_size = vocabulary_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding = nn.Embedding(self.vocabulary_size, self.embedding_size)\n",
    "        self.neural_network = nn.Linear(self.embedding_size, self.vocabulary_size)\n",
    "\n",
    "    def forward(self, input_tensor: torch.Tensor) -> torch.Tensor:\n",
    "        embedding = self.embedding(input_tensor)\n",
    "        embedding = self.neural_network(embedding)\n",
    "        return embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the text dataframe into train, test and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_percent = 60\n",
    "test_percent = 20\n",
    "val_percent = 20\n",
    "\n",
    "train_size = int(len(df) * train_percent / 100) + 1\n",
    "test_size = int(len(df) * test_percent / 100)\n",
    "val_size = int(len(df) * val_percent / 100)\n",
    "\n",
    "len(df) == (train_size + test_size + val_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tokenized_text\"] = df[\"text\"].apply(lambda x: x.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[\"text\"].iloc[:train_size]\n",
    "test_data = df[\"text\"].iloc[train_size: train_size + test_size]\n",
    "val_data = df[\"text\"].iloc[train_size + test_size: train_size + test_size + val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokenized = df[\"tokenized_text\"].iloc[:train_size]\n",
    "test_data_tokenized = df[\"tokenized_text\"].iloc[train_size: train_size + test_size]\n",
    "val_data_tokenized = df[\"tokenized_text\"].iloc[train_size + test_size: train_size + test_size + val_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [@VirginAmerica, plus, you've, added, commerci...\n",
       "1        [@VirginAmerica, it's, really, aggressive, to,...\n",
       "2        [@VirginAmerica, and, it's, a, really, big, ba...\n",
       "3        [@VirginAmerica, seriously, would, pay, $30, a...\n",
       "4        [@VirginAmerica, yes,, nearly, every, time, I,...\n",
       "                               ...                        \n",
       "11536    [@AmericanAir, my, flight, was, Cancelled, Fli...\n",
       "11537    [@AmericanAir, right, on, cue, with, the, dela...\n",
       "11538    [@AmericanAir, thank, you, we, got, on, a, dif...\n",
       "11539    [@AmericanAir, leaving, over, 20, minutes, Lat...\n",
       "11540    [@AmericanAir, you, have, my, money,, you, cha...\n",
       "Name: tokenized_text, Length: 11541, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"tokenized_text\"].iloc[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_tokenized.to_csv(\"../data/train_data.csv\", index=False)\n",
    "test_data_tokenized.to_csv(\"../data/test_data.csv\", index=False)\n",
    "val_data_tokenized.to_csv(\"../data/val_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting our data to HuggingFace Dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-b5bc1ecd6d7ee657\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/anuran/.cache/huggingface/datasets/csv/default-b5bc1ecd6d7ee657/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03668808937072754,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbd8eef5e0e742a7bb2140b0ffb04f78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   "
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.11928987503051758,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting data files #2",
       "rate": null,
       "total": 1,
       "unit": "obj",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f61d19eb8af8413ab5e881167ed870ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #2:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.1509106159210205,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting data files #1",
       "rate": null,
       "total": 1,
       "unit": "obj",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be54c77249784d40aed4c7a63d0cdaa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #1:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.1634678840637207,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Extracting data files #0",
       "rate": null,
       "total": 1,
       "unit": "obj",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aeb647e2c6f492bac626a2a40e61e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files #0:   0%|          | 0/1 [00:00<?, ?obj/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.04025149345397949,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d78d2898ad3444f827ec2324b794bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03190255165100098,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating test split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70b4bebc79164298a75e2f8ee60652de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.040004730224609375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating val split",
       "rate": null,
       "total": 0,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5e1d4d2ef7401b84c041d2c64277f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating val split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/anuran/.cache/huggingface/datasets/csv/default-b5bc1ecd6d7ee657/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.03686666488647461,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 3,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefac1822ca14c5b92bcf3843e39aa35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\"train\": \"../data/train_data.csv\", \"test\": \"../data/test_data.csv\", \"val\": \"../data/val_data.csv\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['tokenized_text'],\n",
       "        num_rows: 6925\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['tokenized_text'],\n",
       "        num_rows: 2308\n",
       "    })\n",
       "    val: Dataset({\n",
       "        features: ['tokenized_text'],\n",
       "        num_rows: 2308\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx2wd = {idx: word for idx, word in enumerate(word_freq_dict.keys())}\n",
    "wd2idx = {word: idx for idx, word in enumerate(word_freq_dict.keys())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: '',\n",
       " 1: 'hey...',\n",
       " 2: '4439',\n",
       " 3: 'num',\n",
       " 4: 'flight🍸#sfo',\n",
       " 5: 'liars!',\n",
       " 6: 'felt,',\n",
       " 7: 'upgd',\n",
       " 8: 'soon?',\n",
       " 9: 'play.',\n",
       " 10: 'promise,',\n",
       " 11: 'glitches',\n",
       " 12: '+1',\n",
       " 13: 'http://t.co/3zpjr7kwbk',\n",
       " 14: 'pos',\n",
       " 15: 'that??',\n",
       " 16: 'offensive.',\n",
       " 17: 'business,',\n",
       " 18: 'sad.',\n",
       " 19: '@ba_usa',\n",
       " 20: '@phd_mama_',\n",
       " 21: '3hours',\n",
       " 22: '2470',\n",
       " 23: 'cheers.',\n",
       " 24: 'frustrating!!',\n",
       " 25: 'news,',\n",
       " 26: 'cared',\n",
       " 27: '#coffeeneeded',\n",
       " 28: '@nickcunningham1',\n",
       " 29: '#isitthegarykellyway?',\n",
       " 30: 'landing,',\n",
       " 31: ';-)',\n",
       " 32: 'subscribe',\n",
       " 33: 'samartzis!',\n",
       " 34: '#fargo',\n",
       " 35: 'http://t.co/2boh2mh3cb',\n",
       " 36: 'leather',\n",
       " 37: 'yogurt',\n",
       " 38: 'flightation',\n",
       " 39: 'yest.',\n",
       " 40: 'warm?',\n",
       " 41: 'fine,',\n",
       " 42: 'another???',\n",
       " 43: 'completely',\n",
       " 44: 'roc',\n",
       " 45: 'switch,',\n",
       " 46: '#americanair',\n",
       " 47: 'direct..bag',\n",
       " 48: '\"a',\n",
       " 49: 'serious.',\n",
       " 50: 'understaffing',\n",
       " 51: 'allow.',\n",
       " 52: '504',\n",
       " 53: 'xoxo',\n",
       " 54: 'suit',\n",
       " 55: '👌👌👌',\n",
       " 56: 'before;',\n",
       " 57: 'you!',\n",
       " 58: 'yvr',\n",
       " 59: '👎😬\\ncustomer',\n",
       " 60: 'message:',\n",
       " 61: '@amexserve',\n",
       " 62: 'http://t.co/xltv5z6st1',\n",
       " 63: 'me?!?!',\n",
       " 64: '\"hope\"',\n",
       " 65: 'crowded',\n",
       " 66: '6:40,',\n",
       " 67: 'http://t.co/1tzz0vbmbs',\n",
       " 68: 'it.',\n",
       " 69: 'flightlation-',\n",
       " 70: '10:30a',\n",
       " 71: 'progress\"',\n",
       " 72: 'future,',\n",
       " 73: 'passes?',\n",
       " 74: 'b737-800w',\n",
       " 75: 'record,',\n",
       " 76: 'degree',\n",
       " 77: 'awesome,',\n",
       " 78: 'danny',\n",
       " 79: 'reasonable!',\n",
       " 80: 'http://t.co/96ftlzwtvo',\n",
       " 81: 'rebecca',\n",
       " 82: 'other.',\n",
       " 83: 'smiling',\n",
       " 84: 'shortly!',\n",
       " 85: 'evrytime',\n",
       " 86: 'acknowledgment.',\n",
       " 87: '\"status\"',\n",
       " 88: 'address,',\n",
       " 89: 'lax-syd.',\n",
       " 90: 'ease.',\n",
       " 91: 'disgusting',\n",
       " 92: 'whenever',\n",
       " 93: 'absolutely',\n",
       " 94: 'vail/eagle.',\n",
       " 95: 'backyard!',\n",
       " 96: 'atlantic',\n",
       " 97: '#jetbluefail',\n",
       " 98: \"i've\",\n",
       " 99: 'deck.',\n",
       " 100: 'potentially',\n",
       " 101: 'orf',\n",
       " 102: '0400.',\n",
       " 103: 'reaching',\n",
       " 104: 'http://t.co/utdfqf5wpa',\n",
       " 105: '@upgrd',\n",
       " 106: '@nelsjeff,',\n",
       " 107: 'tarmac???',\n",
       " 108: 'majority',\n",
       " 109: 'expansion',\n",
       " 110: 'texas!',\n",
       " 111: 'connection.',\n",
       " 112: '\"admirals',\n",
       " 113: 'http://t.co/i7vdi9wqsf',\n",
       " 114: 'rollers',\n",
       " 115: 'copilot',\n",
       " 116: 'spf',\n",
       " 117: 'one',\n",
       " 118: 'beverages.',\n",
       " 119: '@marinadomine',\n",
       " 120: 'steamed',\n",
       " 121: \"doctor's\",\n",
       " 122: '😃cool',\n",
       " 123: 'recovered',\n",
       " 124: 'f@$%ing',\n",
       " 125: '#whyjeff?',\n",
       " 126: '\"loses',\n",
       " 127: 'utah,',\n",
       " 128: 'richard',\n",
       " 129: '#anhour',\n",
       " 130: 'lhr-ord',\n",
       " 131: 'frontrunner',\n",
       " 132: 'http://t.co/gz9gqdt7jj',\n",
       " 133: 'http://t.co/gn30p75kqb',\n",
       " 134: 'indianapolis!',\n",
       " 135: '@kleankanteen',\n",
       " 136: '$285',\n",
       " 137: '\\nthat',\n",
       " 138: 'apologizing',\n",
       " 139: 'unresolved',\n",
       " 140: 'reposting',\n",
       " 141: 'does!',\n",
       " 142: 'wifi#hotel',\n",
       " 143: 'nite',\n",
       " 144: 'happen.',\n",
       " 145: 'alison',\n",
       " 146: 'tails,',\n",
       " 147: '-jfk',\n",
       " 148: 'ewr-ord,',\n",
       " 149: '#lasttweetaboutthis',\n",
       " 150: \"we've\",\n",
       " 151: 'assign',\n",
       " 152: 'destination?',\n",
       " 153: 'cushy',\n",
       " 154: 'prof',\n",
       " 155: 'http://t.co/otty5rymzd',\n",
       " 156: '#dangerofgettingsnowedin',\n",
       " 157: 'sys',\n",
       " 158: 'flts,',\n",
       " 159: 'w/buses',\n",
       " 160: 'beach...',\n",
       " 161: 'formed',\n",
       " 162: 'q4vnmb,',\n",
       " 163: 'luggage!!!',\n",
       " 164: 'painful...',\n",
       " 165: 'gift',\n",
       " 166: 'turned',\n",
       " 167: 'hate',\n",
       " 168: 'remote',\n",
       " 169: 'accept',\n",
       " 170: 'criticism.',\n",
       " 171: 'bag/',\n",
       " 172: '10:10',\n",
       " 173: 'boarded.',\n",
       " 174: 'something.',\n",
       " 175: '25yrs',\n",
       " 176: 'comfortable',\n",
       " 177: 'spoke',\n",
       " 178: '2min',\n",
       " 179: 'tonight!great',\n",
       " 180: 'fend',\n",
       " 181: 'tasha',\n",
       " 182: 'process?',\n",
       " 183: 'free,',\n",
       " 184: '#success',\n",
       " 185: 'incredible',\n",
       " 186: 'went...',\n",
       " 187: 'anytime',\n",
       " 188: 'whose',\n",
       " 189: '703,',\n",
       " 190: '2/15/15',\n",
       " 191: 'early/on',\n",
       " 192: 'http://t.co/clvlhfguzw',\n",
       " 193: '7-10',\n",
       " 194: 'fa',\n",
       " 195: 'lost.',\n",
       " 196: 'circling',\n",
       " 197: 'priority\"',\n",
       " 198: 'count.',\n",
       " 199: 'hangar',\n",
       " 200: 'granting.',\n",
       " 201: 'logged.',\n",
       " 202: '12:17pm.',\n",
       " 203: '0162431184663.\\n3',\n",
       " 204: 'different',\n",
       " 205: 'medically',\n",
       " 206: 'gent',\n",
       " 207: '@jimcramer',\n",
       " 208: 'bgr',\n",
       " 209: 'workng',\n",
       " 210: 'boarding,',\n",
       " 211: 'voicemail.',\n",
       " 212: 'storm!',\n",
       " 213: '1106',\n",
       " 214: '#unitedairlinessuck',\n",
       " 215: 'support!',\n",
       " 216: 'ref#1-2990176298),',\n",
       " 217: 'reduced',\n",
       " 218: 'gang!',\n",
       " 219: '@mallowfairy',\n",
       " 220: 'distance',\n",
       " 221: 'form',\n",
       " 222: 'six.',\n",
       " 223: 'intercom,',\n",
       " 224: 'estimated',\n",
       " 225: 'transferable',\n",
       " 226: 'traveler,',\n",
       " 227: 'pulled',\n",
       " 228: 'early?\"',\n",
       " 229: 'will1531',\n",
       " 230: '(today)',\n",
       " 231: 'bicycle',\n",
       " 232: 'argument',\n",
       " 233: '\\n#onechildfourbags',\n",
       " 234: 'useful.',\n",
       " 235: 'vacation',\n",
       " 236: 'comp',\n",
       " 237: 'early...',\n",
       " 238: 'inquiry',\n",
       " 239: 'b91',\n",
       " 240: '75%',\n",
       " 241: '#mintalicious',\n",
       " 242: 'free\"',\n",
       " 243: 'change/massive',\n",
       " 244: 'bag',\n",
       " 245: 'like',\n",
       " 246: 'compensation?\\n\\ny',\n",
       " 247: 'serious!?',\n",
       " 248: 'cincy',\n",
       " 249: 'costumer',\n",
       " 250: '#nocommunication',\n",
       " 251: 'dime.',\n",
       " 252: 'indy,',\n",
       " 253: '2hr30min',\n",
       " 254: 'va.',\n",
       " 255: '\"economy',\n",
       " 256: 'fail',\n",
       " 257: 'scanned',\n",
       " 258: 'piece',\n",
       " 259: 'sched.',\n",
       " 260: 'b36a.',\n",
       " 261: 'weve',\n",
       " 262: '@marccopely.',\n",
       " 263: 'courtesy?',\n",
       " 264: 'complaints!',\n",
       " 265: 'thousandth',\n",
       " 266: 'couldn’t',\n",
       " 267: 'errors',\n",
       " 268: 'http://t.co/azltjhf7lv.',\n",
       " 269: 'interaction),',\n",
       " 270: 'refunds?',\n",
       " 271: '(vegas',\n",
       " 272: 'de-ice.',\n",
       " 273: 'commercial!',\n",
       " 274: 'case',\n",
       " 275: 'rest',\n",
       " 276: 'abassinet',\n",
       " 277: 'aa375',\n",
       " 278: '#stuckintampa',\n",
       " 279: 'me!!',\n",
       " 280: 'coming!\"',\n",
       " 281: '\"well',\n",
       " 282: '\"broken',\n",
       " 283: 'pimentel',\n",
       " 284: 'tool',\n",
       " 285: 'den.',\n",
       " 286: 'go.......',\n",
       " 287: 'huh',\n",
       " 288: 'display',\n",
       " 289: 'days!#notahappytraveler',\n",
       " 290: 'beating',\n",
       " 291: 'consistently',\n",
       " 292: 'nw',\n",
       " 293: '@jannasaurusrex',\n",
       " 294: 'leinenkugels',\n",
       " 295: 'help?',\n",
       " 296: '#nobackup',\n",
       " 297: 'garbage):',\n",
       " 298: 'relay',\n",
       " 299: 'situation-',\n",
       " 300: '\"eventually',\n",
       " 301: 'bish',\n",
       " 302: 'm.',\n",
       " 303: 'marriott',\n",
       " 304: 'profile?',\n",
       " 305: 'ton!',\n",
       " 306: 'there',\n",
       " 307: 'uncle',\n",
       " 308: 'achieving',\n",
       " 309: 'tallahassee:(',\n",
       " 310: 'changed!',\n",
       " 311: '31',\n",
       " 312: 'minutes.',\n",
       " 313: '70',\n",
       " 314: 'pbi-jfk',\n",
       " 315: 'control???',\n",
       " 316: 'count..',\n",
       " 317: 'stupid',\n",
       " 318: '1547.',\n",
       " 319: 'aisles',\n",
       " 320: '1219.',\n",
       " 321: 'stone',\n",
       " 322: '&lt;--a',\n",
       " 323: 'caribbean',\n",
       " 324: 'phenomenal!',\n",
       " 325: 'kys',\n",
       " 326: 'gloves,',\n",
       " 327: 'say?get',\n",
       " 328: 'ah,',\n",
       " 329: 'club.',\n",
       " 330: '@airtahitinui',\n",
       " 331: '#customer',\n",
       " 332: '7am',\n",
       " 333: 'wendi',\n",
       " 334: 'aircraft!',\n",
       " 335: 'detail',\n",
       " 336: 'fat',\n",
       " 337: '#toughtomakeplans',\n",
       " 338: '(finally)',\n",
       " 339: '1535',\n",
       " 340: 'thanks...seat',\n",
       " 341: 'ids.',\n",
       " 342: 'nit',\n",
       " 343: 'civilized!',\n",
       " 344: 'world?!!!',\n",
       " 345: 'desk',\n",
       " 346: '#kudos.',\n",
       " 347: 'manchester',\n",
       " 348: 'flite',\n",
       " 349: '#nohotel',\n",
       " 350: '@airbus',\n",
       " 351: 'rides',\n",
       " 352: 'basket',\n",
       " 353: 'through?',\n",
       " 354: 'mia.',\n",
       " 355: 'http://t.co/wgyztnjcxm',\n",
       " 356: 'taking.',\n",
       " 357: 'advised',\n",
       " 358: 'gesture',\n",
       " 359: 'ipad',\n",
       " 360: 'breakfast.',\n",
       " 361: '838',\n",
       " 362: 'approach',\n",
       " 363: 'dicks.',\n",
       " 364: 'credited.',\n",
       " 365: 'burned',\n",
       " 366: 'now.frustrated',\n",
       " 367: 'indy',\n",
       " 368: '@usairwayscenter',\n",
       " 369: 'pascucci',\n",
       " 370: 'k13.',\n",
       " 371: 'zone.',\n",
       " 372: '8465981',\n",
       " 373: \"'the\",\n",
       " 374: 'deodorant',\n",
       " 375: 'http://t.co/mraw3qdw4d',\n",
       " 376: 'part.',\n",
       " 377: 'numbers.',\n",
       " 378: 'serving',\n",
       " 379: 'gainesville',\n",
       " 380: 'requested-',\n",
       " 381: 'stressful',\n",
       " 382: 'someone-awesome',\n",
       " 383: '1074',\n",
       " 384: '#mardigras',\n",
       " 385: 'http://t.co/1xzrk66wvq',\n",
       " 386: 'cockroaches',\n",
       " 387: 'hrs,',\n",
       " 388: 'columbia',\n",
       " 389: 'relations...',\n",
       " 390: 'bumping',\n",
       " 391: 'night,',\n",
       " 392: 'layovers/',\n",
       " 393: '@caterobbie',\n",
       " 394: 'earlier!',\n",
       " 395: 'would',\n",
       " 396: 'http://t.co/ypbkcirbxu',\n",
       " 397: '@nanceebing',\n",
       " 398: '#idnumber8569822',\n",
       " 399: 'thing,',\n",
       " 400: 'ahoy',\n",
       " 401: '4.',\n",
       " 402: 'deiced?',\n",
       " 403: '#390',\n",
       " 404: 'lord',\n",
       " 405: 'maintained',\n",
       " 406: '2bestfriends',\n",
       " 407: 'abroad.',\n",
       " 408: 'options?',\n",
       " 409: '#showsomerespect',\n",
       " 410: 'result:',\n",
       " 411: 'hotel.',\n",
       " 412: 'faults.',\n",
       " 413: '#fail',\n",
       " 414: 'trust!',\n",
       " 415: 'fulfilled',\n",
       " 416: '\"dealing',\n",
       " 417: 'dublin.',\n",
       " 418: 'important',\n",
       " 419: 'thoughts:',\n",
       " 420: 'seet',\n",
       " 421: 'candace!',\n",
       " 422: 'tense',\n",
       " 423: 'hasty',\n",
       " 424: 'https://t.co/zgoqoxjbqy',\n",
       " 425: '3x.',\n",
       " 426: 'belief',\n",
       " 427: 'useless.',\n",
       " 428: 'mci&gt;dfw.',\n",
       " 429: 'terrible!',\n",
       " 430: 'ozs.',\n",
       " 431: 'effing',\n",
       " 432: 'gives',\n",
       " 433: '#whrsthecoach?',\n",
       " 434: 'awol',\n",
       " 435: 'replied,',\n",
       " 436: '1729?',\n",
       " 437: 'vacationing',\n",
       " 438: 'southwest',\n",
       " 439: 'attempts,',\n",
       " 440: 'funited',\n",
       " 441: 'classes',\n",
       " 442: 'any?',\n",
       " 443: 'pr,',\n",
       " 444: 'happy,',\n",
       " 445: '\"processing',\n",
       " 446: 'lbs',\n",
       " 447: '3/5.',\n",
       " 448: 'horrid.',\n",
       " 449: 'assignments.',\n",
       " 450: 'fee:',\n",
       " 451: 'gas.',\n",
       " 452: '#flyerfriendly',\n",
       " 453: 'phoenix?',\n",
       " 454: 'http://t.co/gcwvfuopl2',\n",
       " 455: 'tuesday!',\n",
       " 456: 'hole..',\n",
       " 457: 'snowbound',\n",
       " 458: 'evening',\n",
       " 459: '@jimdaytv',\n",
       " 460: 'gassing',\n",
       " 461: 'landing?',\n",
       " 462: 'beers',\n",
       " 463: '#sohappy',\n",
       " 464: '#lhrt2',\n",
       " 465: 'where...',\n",
       " 466: 'desired',\n",
       " 467: '#smitten',\n",
       " 468: 'although',\n",
       " 469: '#facepalm',\n",
       " 470: 'bdl-dca',\n",
       " 471: 'french',\n",
       " 472: '#bebetter',\n",
       " 473: 'funny!',\n",
       " 474: 'min.',\n",
       " 475: 'mpwnc2.',\n",
       " 476: 'uncomfortable,',\n",
       " 477: 'http://t.co/ac6zwmuoon”',\n",
       " 478: 'ua3710',\n",
       " 479: '1614',\n",
       " 480: '(atl-ord)',\n",
       " 481: 'twitter/dm.',\n",
       " 482: 'superior',\n",
       " 483: 'trained',\n",
       " 484: 'cana',\n",
       " 485: 'ups',\n",
       " 486: 'preferred',\n",
       " 487: 'mins,',\n",
       " 488: \"'on\",\n",
       " 489: '5491',\n",
       " 490: 'throw',\n",
       " 491: 'bwahahaha!',\n",
       " 492: 'bag!!!!',\n",
       " 493: 'good!!!💙💙💙💙',\n",
       " 494: 'terrible,',\n",
       " 495: '2258',\n",
       " 496: 'galaxy',\n",
       " 497: 'common?',\n",
       " 498: 'happened,',\n",
       " 499: '#shameful',\n",
       " 500: 'delays.handling',\n",
       " 501: 'grinding.',\n",
       " 502: 'arrives,',\n",
       " 503: 'flightrs',\n",
       " 504: '#us3645',\n",
       " 505: 'http://t.co/mvyoizrpde',\n",
       " 506: '#badbusiness',\n",
       " 507: 'headaches.',\n",
       " 508: '1777',\n",
       " 509: '#usairwaysfail',\n",
       " 510: 'http://t.co/m9nywr5kbs',\n",
       " 511: 'impending',\n",
       " 512: 'supporter',\n",
       " 513: 'pointy',\n",
       " 514: 'order',\n",
       " 515: 'delay!!',\n",
       " 516: 'compensates',\n",
       " 517: 'around',\n",
       " 518: 'whoooo',\n",
       " 519: 'chance?',\n",
       " 520: 'issues\"',\n",
       " 521: 'forever?',\n",
       " 522: 'inquire.',\n",
       " 523: 'virtually',\n",
       " 524: 'rookie.',\n",
       " 525: 'promptly.',\n",
       " 526: 'lol',\n",
       " 527: '(aa200)',\n",
       " 528: '#mosaicmecrazy',\n",
       " 529: 'sea,',\n",
       " 530: '#getmeoffthisplane',\n",
       " 531: 'c',\n",
       " 532: 'duped',\n",
       " 533: '✈️🌞',\n",
       " 534: 'http://t.co/maqw2nlniu',\n",
       " 535: 'w/small',\n",
       " 536: '#tired&amp;frustrated',\n",
       " 537: 'http://t.co/egkvfokogj',\n",
       " 538: 'produce',\n",
       " 539: '#virginamerica',\n",
       " 540: 'holy,',\n",
       " 541: 'columbus!!!!!!',\n",
       " 542: '$475',\n",
       " 543: '.other',\n",
       " 544: '#5102newflight',\n",
       " 545: 'fly,',\n",
       " 546: 'season,',\n",
       " 547: 'sporadically',\n",
       " 548: '😡😡',\n",
       " 549: '(although',\n",
       " 550: 'happened\"',\n",
       " 551: 'contact',\n",
       " 552: 'ua3882',\n",
       " 553: 'ads',\n",
       " 554: '#aadelay',\n",
       " 555: '#lostsuitcase',\n",
       " 556: 'slash',\n",
       " 557: 'hour--not',\n",
       " 558: 'k20',\n",
       " 559: 'lady,',\n",
       " 560: 'reps',\n",
       " 561: '#systemwide',\n",
       " 562: 'yesterday;',\n",
       " 563: 'girlfriend’s',\n",
       " 564: 'waste',\n",
       " 565: '@southwestair.',\n",
       " 566: 'tones',\n",
       " 567: 'representative,',\n",
       " 568: 'ua82',\n",
       " 569: 'intl',\n",
       " 570: 'sentiment',\n",
       " 571: 'passengers-',\n",
       " 572: 'tv?!?!',\n",
       " 573: 'but,',\n",
       " 574: 'doumented',\n",
       " 575: 'pts',\n",
       " 576: 'mins…',\n",
       " 577: 'lga-iah',\n",
       " 578: 'problem...',\n",
       " 579: 'tug',\n",
       " 580: 'seriously,',\n",
       " 581: \"baggage's\",\n",
       " 582: 'aware,',\n",
       " 583: 'unexpected.',\n",
       " 584: 'path,',\n",
       " 585: 'nature',\n",
       " 586: 'circle',\n",
       " 587: 'planning.',\n",
       " 588: 'tue',\n",
       " 589: 'today...this',\n",
       " 590: 'availability.',\n",
       " 591: 'ord,',\n",
       " 592: 'touched',\n",
       " 593: 'unknown',\n",
       " 594: 'pqd',\n",
       " 595: '634',\n",
       " 596: '#wheresmyrefund',\n",
       " 597: '#awful',\n",
       " 598: 'msp',\n",
       " 599: 'period',\n",
       " 600: 'confirmed\"',\n",
       " 601: '8:05am',\n",
       " 602: '...you',\n",
       " 603: 'chuckhole',\n",
       " 604: '5302',\n",
       " 605: 'right.',\n",
       " 606: 'worked.',\n",
       " 607: 'centre?',\n",
       " 608: 'hope?',\n",
       " 609: 'airport!',\n",
       " 610: 'up…',\n",
       " 611: 'hi.',\n",
       " 612: 'failures.',\n",
       " 613: '@delta.',\n",
       " 614: 'delayed/cancelled',\n",
       " 615: 'reopened',\n",
       " 616: 'c4.',\n",
       " 617: 'regret',\n",
       " 618: '\"rather',\n",
       " 619: 'communicate',\n",
       " 620: 'respond,',\n",
       " 621: 'manch,',\n",
       " 622: '100',\n",
       " 623: 'proceed',\n",
       " 624: 'question',\n",
       " 625: 'http://t.co/fupf0uayir',\n",
       " 626: '$299.',\n",
       " 627: 'disappointed\"',\n",
       " 628: '#frozentoilet',\n",
       " 629: 'experience?',\n",
       " 630: 'http://t.co/loeco4gmvd',\n",
       " 631: 'erw',\n",
       " 632: 're-cheduled',\n",
       " 633: 'fucked...',\n",
       " 634: 'sign,',\n",
       " 635: 'avail\"',\n",
       " 636: 'award.',\n",
       " 637: 'skycaps,',\n",
       " 638: 'reflight',\n",
       " 639: '#mechanicalissue',\n",
       " 640: 'navigate',\n",
       " 641: 'station',\n",
       " 642: 'unfortunate.',\n",
       " 643: '..i',\n",
       " 644: 'screens,',\n",
       " 645: 'game.',\n",
       " 646: 'absolutely!',\n",
       " 647: '$179',\n",
       " 648: 'hold\"',\n",
       " 649: 'lay',\n",
       " 650: 'nexus',\n",
       " 651: 'hmmm.',\n",
       " 652: 'calls',\n",
       " 653: '#cancun',\n",
       " 654: '0671',\n",
       " 655: 'appalling.',\n",
       " 656: 'stuff?',\n",
       " 657: 'lga',\n",
       " 658: '#nolove',\n",
       " 659: '250',\n",
       " 660: 'http://t.co/6kp4m0r1f7',\n",
       " 661: 'flightncy)',\n",
       " 662: 'fast',\n",
       " 663: 'nationalized',\n",
       " 664: 'holders',\n",
       " 665: '.@virginamerica',\n",
       " 666: 'established',\n",
       " 667: 'luggages',\n",
       " 668: '#outoftouchwithreality',\n",
       " 669: 'convo',\n",
       " 670: '#legroom',\n",
       " 671: '23',\n",
       " 672: 'center',\n",
       " 673: 'map?',\n",
       " 674: 'unlucky',\n",
       " 675: 'greatest!',\n",
       " 676: 'please....how',\n",
       " 677: 'rampers',\n",
       " 678: '#poorplanning',\n",
       " 679: 'apologizes',\n",
       " 680: 'min....',\n",
       " 681: 'cr?',\n",
       " 682: 'prohibits',\n",
       " 683: 'employees..lack',\n",
       " 684: 'honest,',\n",
       " 685: '@tonysimsmma',\n",
       " 686: 'jose.',\n",
       " 687: 'ill',\n",
       " 688: 'completely.',\n",
       " 689: 'however;',\n",
       " 690: 'performing',\n",
       " 691: 'bag!!',\n",
       " 692: 'ua1589,',\n",
       " 693: 'exactly!',\n",
       " 694: 'game',\n",
       " 695: 'hardworking',\n",
       " 696: 'http://t.co/8vnckgzxl1',\n",
       " 697: 'bergstrom)',\n",
       " 698: 'unlike',\n",
       " 699: 'soon.\"',\n",
       " 700: '8:55pm',\n",
       " 701: 'land!',\n",
       " 702: 'pit/iad,',\n",
       " 703: 'heck...booked',\n",
       " 704: 'alcohol',\n",
       " 705: 'francisco',\n",
       " 706: 'honeymoon...',\n",
       " 707: 'jerk.',\n",
       " 708: 'chicago',\n",
       " 709: 'cud',\n",
       " 710: 'mileageplus',\n",
       " 711: \"couldn't.\",\n",
       " 712: 'responses',\n",
       " 713: '@united!',\n",
       " 714: 'way!!💺✈️',\n",
       " 715: '#goodluckamericanair',\n",
       " 716: 'http://t.co/a0yosjhzmc',\n",
       " 717: '-_-',\n",
       " 718: 'thing!',\n",
       " 719: 'airport..2',\n",
       " 720: 'gate???',\n",
       " 721: 'flightr.',\n",
       " 722: 'kiosks',\n",
       " 723: 'understanding',\n",
       " 724: '689',\n",
       " 725: 'birthday,',\n",
       " 726: 'normally.',\n",
       " 727: 'perfect..',\n",
       " 728: 'hold...',\n",
       " 729: 'bulkhead',\n",
       " 730: 'city,',\n",
       " 731: 'it-',\n",
       " 732: 'http://t.co/csddccmvbd',\n",
       " 733: 'cstmr',\n",
       " 734: '#wasteoftime',\n",
       " 735: 'mainline',\n",
       " 736: 'delayed,',\n",
       " 737: 'transactional',\n",
       " 738: '4972',\n",
       " 739: 'reassess',\n",
       " 740: 'explains',\n",
       " 741: '2/11/15!',\n",
       " 742: 'cos',\n",
       " 743: '3349,',\n",
       " 744: '#flysouthwest',\n",
       " 745: '@josephtreis',\n",
       " 746: 'they',\n",
       " 747: 'finger',\n",
       " 748: 'cmh',\n",
       " 749: '\"goodwill',\n",
       " 750: 'inconvenience!!!!!!',\n",
       " 751: 'discover',\n",
       " 752: 'powered',\n",
       " 753: 'den:phx.',\n",
       " 754: 'chat.',\n",
       " 755: 'me!',\n",
       " 756: 'http://t.co/6kqlhvap7g',\n",
       " 757: 'to\"arrange',\n",
       " 758: 'ua1416',\n",
       " 759: 'tomorrows',\n",
       " 760: '⤵for',\n",
       " 761: 'programming',\n",
       " 762: 'anywhere,',\n",
       " 763: '✌️out.',\n",
       " 764: '2yr',\n",
       " 765: 'broken?',\n",
       " 766: '#daddyshome',\n",
       " 767: 'girlfriend',\n",
       " 768: 'servicing',\n",
       " 769: 'schedule...',\n",
       " 770: 'feels',\n",
       " 771: 'pandu',\n",
       " 772: 'paris.',\n",
       " 773: 'help',\n",
       " 774: 'questions!',\n",
       " 775: '#currentlysittingontarmac',\n",
       " 776: 'http://t.co/tkvcmhbpim',\n",
       " 777: \"she'd\",\n",
       " 778: 'recovering',\n",
       " 779: '#lazy',\n",
       " 780: 'spread',\n",
       " 781: 'bench',\n",
       " 782: 'early,connecting',\n",
       " 783: 'told',\n",
       " 784: 'destroyed',\n",
       " 785: 'how/why',\n",
       " 786: 'fll',\n",
       " 787: '#dreamliner',\n",
       " 788: '380',\n",
       " 789: '&lt;3',\n",
       " 790: 'situation!',\n",
       " 791: '#stillmakingmepoorthough',\n",
       " 792: 'families.',\n",
       " 793: 'sw,',\n",
       " 794: 'fewer',\n",
       " 795: '1800#.',\n",
       " 796: 'slower.',\n",
       " 797: 'b)',\n",
       " 798: '@derekc21',\n",
       " 799: '@',\n",
       " 800: 'pia!',\n",
       " 801: '@lpalumbo',\n",
       " 802: 'ebhset.',\n",
       " 803: '#luvintheair',\n",
       " 804: 'settled',\n",
       " 805: 'selves',\n",
       " 806: '#patience',\n",
       " 807: '#theydontanswer',\n",
       " 808: 'http://t.co/mb3ipgs8zq”',\n",
       " 809: 'quickly!!',\n",
       " 810: '\\nmia-sfo',\n",
       " 811: 'http://t.co/docmvotwti',\n",
       " 812: '(it',\n",
       " 813: 'employee.working',\n",
       " 814: 'http://t.co/rhkamx9vf5.',\n",
       " 815: 'said,',\n",
       " 816: '1514',\n",
       " 817: 'tell/purchase',\n",
       " 818: 'swell',\n",
       " 819: 'interim',\n",
       " 820: '#us2218',\n",
       " 821: '#noloveforsurfers',\n",
       " 822: 'diversion.',\n",
       " 823: 'lame',\n",
       " 824: '#greatcustomerservice✈☺',\n",
       " 825: 'thru-',\n",
       " 826: 'us2118\\nmy',\n",
       " 827: 'request:',\n",
       " 828: 'us\"',\n",
       " 829: 'picks',\n",
       " 830: 'ord???',\n",
       " 831: 'ua1764',\n",
       " 832: 'ua1665',\n",
       " 833: 'submitted',\n",
       " 834: '350pm',\n",
       " 835: 'help!!!',\n",
       " 836: 'customers',\n",
       " 837: '@momsgoodeats',\n",
       " 838: 'robot.',\n",
       " 839: 'raleigh?',\n",
       " 840: 'https://t.co/2pujvcelng',\n",
       " 841: 'canx',\n",
       " 842: 'un',\n",
       " 843: 'pilots!',\n",
       " 844: 'actions?',\n",
       " 845: 'jewel',\n",
       " 846: 'pleasure,',\n",
       " 847: '&gt;1hr',\n",
       " 848: '#youareonyourown',\n",
       " 849: '#moodlitmonday',\n",
       " 850: '@cbcallinaday',\n",
       " 851: 'okay...',\n",
       " 852: 'http://t.co/atd2sm8hf4',\n",
       " 853: 'language',\n",
       " 854: 'dunkin',\n",
       " 855: 'far.',\n",
       " 856: 'voted',\n",
       " 857: 'toddlers',\n",
       " 858: 'look?',\n",
       " 859: 'friends!',\n",
       " 860: '@cinnabon',\n",
       " 861: 'swapping',\n",
       " 862: 'exciting',\n",
       " 863: 'anything\"',\n",
       " 864: 'island',\n",
       " 865: 'a-ok',\n",
       " 866: 'fixing',\n",
       " 867: 'name,',\n",
       " 868: 'previous',\n",
       " 869: 'airlines...',\n",
       " 870: 'horrible!',\n",
       " 871: 'cun?',\n",
       " 872: '1+',\n",
       " 873: '4465',\n",
       " 874: '80th',\n",
       " 875: '0',\n",
       " 876: '1776',\n",
       " 877: '#1623',\n",
       " 878: 'pending.',\n",
       " 879: 'brilliant!',\n",
       " 880: '@rockinwellness',\n",
       " 881: 'obey',\n",
       " 882: 'listening!',\n",
       " 883: 'advertising,',\n",
       " 884: 'processes.',\n",
       " 885: 'cave',\n",
       " 886: 'employee,',\n",
       " 887: 'lloyd',\n",
       " 888: 'june.',\n",
       " 889: 'clockwork,',\n",
       " 890: 'flightling',\n",
       " 891: '#nofunatall',\n",
       " 892: 'and/or',\n",
       " 893: 'a.',\n",
       " 894: 'eyw.',\n",
       " 895: 'april,',\n",
       " 896: 'ems.',\n",
       " 897: 'flightled.',\n",
       " 898: '(sent',\n",
       " 899: 'http://t.co/3x9nruovts',\n",
       " 900: 'doll',\n",
       " 901: 'air',\n",
       " 902: 'pissed/very',\n",
       " 903: 'unbelievable.',\n",
       " 904: 'holton',\n",
       " 905: 'said!',\n",
       " 906: 'ahah😃💕🎵',\n",
       " 907: '@nathankillam',\n",
       " 908: 'vegas!',\n",
       " 909: 'yo,',\n",
       " 910: 'know-',\n",
       " 911: 'noodles,',\n",
       " 912: 'mellie',\n",
       " 913: '#avgeek',\n",
       " 914: 'updates,',\n",
       " 915: 'longer,',\n",
       " 916: 'dice',\n",
       " 917: 'available.your',\n",
       " 918: 'https://t.co/caf2cx3gfi',\n",
       " 919: 'talks',\n",
       " 920: '2202',\n",
       " 921: '#489.',\n",
       " 922: 'http://t.co/vudwjm1lyb',\n",
       " 923: '4285.',\n",
       " 924: '#best!',\n",
       " 925: 'marisol',\n",
       " 926: 'department.',\n",
       " 927: 'cling',\n",
       " 928: 'wc',\n",
       " 929: 'fritz,',\n",
       " 930: 'luxurious',\n",
       " 931: 'care!!!\"',\n",
       " 932: 'answers....',\n",
       " 933: 'disaster.',\n",
       " 934: 'ricardo',\n",
       " 935: 'fib',\n",
       " 936: '(via',\n",
       " 937: 'tongue-in-cheek',\n",
       " 938: 'forms',\n",
       " 939: 'rum!',\n",
       " 940: '1172.',\n",
       " 941: '#clockwork',\n",
       " 942: '02/14',\n",
       " 943: 'plus',\n",
       " 944: 'hospice',\n",
       " 945: 'designed',\n",
       " 946: 'helpful?',\n",
       " 947: 'y/o',\n",
       " 948: 'bereavement,',\n",
       " 949: '4125',\n",
       " 950: \"aren't\",\n",
       " 951: '#roadwarrior',\n",
       " 952: 'miscounting',\n",
       " 953: 'birthday!!!!',\n",
       " 954: '2:',\n",
       " 955: 'sense?',\n",
       " 956: 'aggravating.',\n",
       " 957: '\"do',\n",
       " 958: 'scl-',\n",
       " 959: 'tailwind.',\n",
       " 960: 'bone',\n",
       " 961: '#greed',\n",
       " 962: 'stray',\n",
       " 963: '#heartlanta',\n",
       " 964: 'sundays',\n",
       " 965: 'alive?',\n",
       " 966: 'till',\n",
       " 967: 'prev.',\n",
       " 968: 'pops',\n",
       " 969: 'now!',\n",
       " 970: 'dealings',\n",
       " 971: '2a',\n",
       " 972: 'http://t.co/u6duw27mde.',\n",
       " 973: 'break.',\n",
       " 974: '6wks',\n",
       " 975: 'tx.',\n",
       " 976: 'crutches',\n",
       " 977: 'jetblue👌☺️',\n",
       " 978: 'http://t.co/ox4w6ktsgi',\n",
       " 979: '1other',\n",
       " 980: 'trials',\n",
       " 981: 'slc.',\n",
       " 982: '\"buy\"',\n",
       " 983: 'case.',\n",
       " 984: 'chase',\n",
       " 985: 'unloading',\n",
       " 986: 'allergies',\n",
       " 987: 'priorities!!🌟🌟',\n",
       " 988: '\"chicago\\'s',\n",
       " 989: 'yes...',\n",
       " 990: 'why\\ni',\n",
       " 991: 'inappropriate',\n",
       " 992: 'lax-iad',\n",
       " 993: 'boss',\n",
       " 994: 'unfollowing,',\n",
       " 995: 'belt!!!!!#toolong',\n",
       " 996: 'spot',\n",
       " 997: 'represents',\n",
       " 998: '95',\n",
       " 999: 'prepared.',\n",
       " ...}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx2wd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate a Word2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Word2VecSkipgramModel(vocabulary_size=len(train_data), embedding_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
