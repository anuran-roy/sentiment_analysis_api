{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setup complete.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "import re\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "print(\"Setup complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica plus you've added commercials t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica it's really aggressive to blast...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica and it's a really big bad thing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>@VirginAmerica seriously would pay $30 a fligh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>@VirginAmerica yes, nearly every time I fly VX...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0          positive  @VirginAmerica plus you've added commercials t...\n",
       "1          negative  @VirginAmerica it's really aggressive to blast...\n",
       "2          negative  @VirginAmerica and it's a really big bad thing...\n",
       "3          negative  @VirginAmerica seriously would pay $30 a fligh...\n",
       "4          positive  @VirginAmerica yes, nearly every time I fly VX..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data = pd.read_csv(\"../data/airline_sentiment_analysis.csv\").drop(labels=[\"Unnamed: 0\"], axis=1)\n",
    "sentiment_data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive comments =  airline_sentiment    2363\n",
      "text                 2363\n",
      "dtype: int64\n",
      "Negative comments =  airline_sentiment    9178\n",
      "text                 9178\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive comments = \", sentiment_data[sentiment_data[\"airline_sentiment\"] == \"positive\"].count())\n",
    "print(\"Negative comments = \", sentiment_data[sentiment_data[\"airline_sentiment\"] == \"negative\"].count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence we can see that there is a huge bias here- the number of negative comments is far more than the positive ones. So we sample out 2363 from them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "def remove_abbreviations(sentence: str) -> str:\n",
    "    abb_dict: Dict[str, str] = {\n",
    "        \"i'm\": \"i am\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"i'd\": \"i would\",\n",
    "        \"i'll\": \"i will\",\n",
    "        \"he's\": \"he is\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"ain't\": \"am not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll've\": \"he will have\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"i'd've\": \"i would have\",\n",
    "        \"i'll've\": \"i will have\",\n",
    "        \"i'm'a\": \"i am about to\",\n",
    "        \"i'm'o\": \"i am going to\",\n",
    "        \"i've\": \"i have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"'em\": \"them\",\n",
    "        \"wanna\": \"want to\",\n",
    "        \"gonna\": \"going to\",\n",
    "        \"gotta\": \"got to\",\n",
    "        \"lemme\": \"let me\",\n",
    "        \"more'n\": \"more than\",\n",
    "        \"'bout\": \"about\",\n",
    "        \"'til\": \"until\",\n",
    "        \"kinda\": \"kind of\",\n",
    "        \"sorta\": \"sort of\",\n",
    "        \"lotta\": \"lot of\",\n",
    "        \"aught\": \"ought\",\n",
    "        \"methinks\": \"me thinks\",\n",
    "        \"methinks\": \"me thinks\",\n",
    "        \"o'er\": \"over\",\n",
    "        \"tis\": \"it is\",\n",
    "        \"tisn't\": \"it is not\",\n",
    "        \"twas\": \"it was\",\n",
    "        \"twasn't\": \"it was not\",\n",
    "        \"wot\": \"what\",\n",
    "        \"wotcha\": \"what are you\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"you've\": \"you have\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"i've\": \"i have\",\n",
    "    }\n",
    "\n",
    "    for abb, full in abb_dict.items():\n",
    "        sentence = sentence.replace(abb, full)\n",
    "    \n",
    "    return sentence\n",
    "\n",
    "def preprocess_sentence(sentence: str) -> str:\n",
    "    sentence = sentence.lower()\n",
    "    sentence = remove_abbreviations(sentence)\n",
    "    sentence = re.sub(r'\\W', ' ', str(sentence))\n",
    "\n",
    "    # remove all single characters\n",
    "    sentence= re.sub(r'\\s+[a-zA-Z]\\s+', ' ', sentence)\n",
    "\n",
    "    # Remove single characters from the start\n",
    "    sentence = re.sub(r'\\^[a-zA-Z]\\s+', ' ', sentence) \n",
    "\n",
    "    # Substituting multiple spaces with single space\n",
    "    sentence = re.sub(r'\\s+', ' ', sentence, flags=re.I)\n",
    "\n",
    "    # Removing prefixed 'b'\n",
    "    sentence = re.sub(r'^b\\s+', '', sentence)\n",
    "\n",
    "    # Converting to Lowercase\n",
    "    sentence = sentence.lower()\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>airline_sentiment</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica plus you have added commercials...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica it is really aggressive to blas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica and it is really big bad thing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>virginamerica seriously would pay 30 flight f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>positive</td>\n",
       "      <td>virginamerica yes nearly every time fly vx th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  airline_sentiment                                               text\n",
       "0          positive   virginamerica plus you have added commercials...\n",
       "1          negative   virginamerica it is really aggressive to blas...\n",
       "2          negative   virginamerica and it is really big bad thing ...\n",
       "3          negative   virginamerica seriously would pay 30 flight f...\n",
       "4          positive   virginamerica yes nearly every time fly vx th..."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_data[\"text\"] = sentiment_data[\"text\"].apply(preprocess_sentence)\n",
    "sentiment_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = None\n",
    "\n",
    "def get_tf_idf(sentences: pd.Series) -> pd.DataFrame:\n",
    "    global vectorizer\n",
    "    print(\"Input type =\", type(sentences))\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    vectorizer.fit(sentences)\n",
    "    response = vectorizer.transform(sentences).toarray()\n",
    "    print(\"Output Type = \", type(response))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type = <class 'pandas.core.series.Series'>\n",
      "Output Type =  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "result = get_tf_idf(sentiment_data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = sentiment_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(result, sentiment_data[\"airline_sentiment\"], test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_classifier = RandomForestClassifier(random_state=42)\n",
    "text_classifier.fit(X_train, y_train)\n",
    "text_classifier.fit(X_train, y_train)\n",
    "# cv_results = cross_validate(text_classifier, X_test, y_test, cv=5)\n",
    "\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = text_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2363, 9178]\n",
      "2363\n",
      "[2363, 3059]\n",
      "[1, 3]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anuran/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:455: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\n",
      "/home/anuran/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:455: UserWarning: Warm-start fitting without increasing n_estimators does not fit new trees.\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "\n",
    "def train_data_iter(model, class_sampled_data: List[pd.DataFrame]=[]):\n",
    "    dataset_sizes = [len(class_sampled_data[i]) for i in range(len(class_sampled_data))]\n",
    "    print(dataset_sizes)\n",
    "    min_len = min(dataset_sizes)\n",
    "    print(min_len)\n",
    "    slice_sizes = [(x//(x // min_len + (x % min_len) // min_len)) for x in dataset_sizes]\n",
    "    print(slice_sizes)\n",
    "    slices_counts = [(x // min_len + (x % min_len) // min_len) for x in dataset_sizes]\n",
    "    print(slices_counts)\n",
    "    max_iters = max(slices_counts)\n",
    "\n",
    "    for i in range(max_iters):\n",
    "        model.set_params(n_estimators=200, warm_start=True)\n",
    "        training_data = pd.concat([class_sampled_data[i].sample(slice_sizes[i]) for i in range(len(class_sampled_data))], axis=0, ignore_index=True)\n",
    "\n",
    "        model.fit([training_data[\"text\"]], [training_data[\"airline_sentiment\"]])\n",
    "\n",
    "    return model\n",
    "\n",
    "df2 = sentiment_data.copy()\n",
    "df2[\"text\"] = result\n",
    "model = train_data_iter(RandomForestClassifier(warm_start=True, random_state=42, n_estimators=200), [df2[df2[\"airline_sentiment\"] == \"positive\"], df2[df2[\"airline_sentiment\"] == \"negative\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input type = <class 'pandas.core.series.Series'>\n",
      "Output Type =  <class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 12637 features, but RandomForestClassifier is expecting 5422 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/media/anuran/Samsung SSD 970 EVO 1TB/Internship/TrueFoundry/Internship Task/dummy/sklearn_sentiment_analysis.ipynb Cell 17\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/media/anuran/Samsung%20SSD%20970%20EVO%201TB/Internship/TrueFoundry/Internship%20Task/dummy/sklearn_sentiment_analysis.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m predictions \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(get_tf_idf(sentiment_data[\u001b[39m\"\u001b[39;49m\u001b[39mtext\u001b[39;49m\u001b[39m\"\u001b[39;49m]))\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/anuran/Samsung%20SSD%20970%20EVO%201TB/Internship/TrueFoundry/Internship%20Task/dummy/sklearn_sentiment_analysis.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(confusion_matrix(sentiment_data[\u001b[39m\"\u001b[39m\u001b[39mairline_sentiment\u001b[39m\u001b[39m\"\u001b[39m],predictions))\n\u001b[1;32m      <a href='vscode-notebook-cell:/media/anuran/Samsung%20SSD%20970%20EVO%201TB/Internship/TrueFoundry/Internship%20Task/dummy/sklearn_sentiment_analysis.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test,predictions))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:832\u001b[0m, in \u001b[0;36mForestClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, X):\n\u001b[1;32m    812\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39m    Predict class for X.\u001b[39;00m\n\u001b[1;32m    814\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    830\u001b[0m \u001b[39m        The predicted classes.\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 832\u001b[0m     proba \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict_proba(X)\n\u001b[1;32m    834\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_outputs_ \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    835\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39margmax(proba, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m), axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:874\u001b[0m, in \u001b[0;36mForestClassifier.predict_proba\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    872\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m    873\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[0;32m--> 874\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[1;32m    876\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[1;32m    877\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/ensemble/_forest.py:605\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    602\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    603\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[1;32m    604\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m--> 605\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    606\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[1;32m    607\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:600\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    597\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[1;32m    599\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[0;32m--> 600\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[1;32m    602\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[0;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    403\u001b[0m     )\n",
      "\u001b[0;31mValueError\u001b[0m: X has 12637 features, but RandomForestClassifier is expecting 5422 features as input."
     ]
    }
   ],
   "source": [
    "\n",
    "predictions = model.predict(get_tf_idf(sentiment_data[\"text\"]))\n",
    "\n",
    "print(confusion_matrix(sentiment_data[\"airline_sentiment\"],predictions))\n",
    "print(classification_report(y_test,predictions))\n",
    "print(accuracy_score(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'negative', 'negative', ..., 'negative', 'negative',\n",
       "       'negative'], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/anuran/Samsung SSD 970 EVO 1TB/Internship/TrueFoundry/Internship Task\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "BASE_DIR = Path(os.path.abspath(\".\")).parent.absolute()\n",
    "print(BASE_DIR)\n",
    "def save_model(loc: str = f\"{BASE_DIR}/backend/api/models/scikit_learn/weights/classifier/best.pkl\"):\n",
    "    with open(loc, \"wb\") as fout:\n",
    "        pickle.dump(text_classifier, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"{BASE_DIR}/backend/api/models/scikit_learn/weights/vectorizer/best.pkl\", \"wb\") as fout:\n",
    "    pickle.dump(vectorizer, fout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
